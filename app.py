import streamlit as st
import google.generativeai as genai
import PyPDF2
from docx import Document
import os

# 1. API í‚¤ ì„¤ì •
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
    st.stop()

# 2. í™”ë©´ ì„¤ì •
st.set_page_config(page_title="4.ìš°ë¦¬ ëª¸ì˜ êµ¬ì¡°ì™€ ê¸°ëŠ¥", page_icon="ğŸ©º")
st.title("4.ìš°ë¦¬ ëª¸ì˜ êµ¬ì¡°ì™€ ê¸°ëŠ¥")
st.caption("ì„ ìƒë‹˜ê³¼ í•¨ê»˜ ìš°ë¦¬ ëª¸ì— ëŒ€í•´ ì¬ë¯¸ìˆê²Œ ì•Œì•„ë³´ì•„ìš”!")

# 3. ëª¨ë¸ ì—°ê²° (ì„ ìƒë‹˜ ìš”ì²­: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì§ì ‘ íƒìƒ‰ ë°©ì‹ â­)
@st.cache_resource
def get_model():
    genai.configure(api_key=GOOGLE_API_KEY)
    
    selected_model = None
    connected_name = ""
    
    try:
        # [í•µì‹¬ ê¸°ëŠ¥] ë‚´ ê³„ì •ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
        # "generateContent" (ëŒ€í™” ê¸°ëŠ¥)ë¥¼ ì§€ì›í•˜ëŠ” ë†ˆë“¤ë§Œ ì¶”ë ¤ëƒ…ë‹ˆë‹¤.
        my_available_models = []
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                my_available_models.append(m)

        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ í•˜ë‚˜ë„ ì—†ë‹¤ë©´?
        if not my_available_models:
            return None, "ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"

        # [ë˜‘ë˜‘í•œ ì„ íƒ ì „ëµ]
        # ì¡°íšŒëœ ëª©ë¡(my_available_models) ì¤‘ì—ì„œ ê°€ì¥ ì¢‹ì€ ê±¸ ìˆœì„œëŒ€ë¡œ ì°¾ìŠµë‹ˆë‹¤.
        
        # 1ìˆœìœ„: 1.5 Flash (ë¹ ë¥´ê³  ìµœì‹ )
        for m in my_available_models:
            if 'gemini-1.5-flash' in m.name:
                selected_model = genai.GenerativeModel(m.name)
                connected_name = m.name
                break
        
        # 1ìˆœìœ„ê°€ ì—†ìœ¼ë©´ -> 2ìˆœìœ„: 1.5 Pro (ë˜‘ë˜‘í•¨)
        if selected_model is None:
            for m in my_available_models:
                if 'gemini-1.5-pro' in m.name:
                    selected_model = genai.GenerativeModel(m.name)
                    connected_name = m.name
                    break
        
        # 2ìˆœìœ„ë„ ì—†ìœ¼ë©´ -> 3ìˆœìœ„: ê·¸ëƒ¥ Gemini Pro
        if selected_model is None:
            for m in my_available_models:
                if 'gemini-pro' in m.name:
                    selected_model = genai.GenerativeModel(m.name)
                    connected_name = m.name
                    break
        
        # ì•„ë¬´ê²ƒë„ ë§¤ì¹­ì´ ì•ˆ ë˜ë©´ -> ê·¸ëƒ¥ ëª©ë¡ì˜ ì²« ë²ˆì§¸ ë†ˆì„ ë¬´ì¡°ê±´ ì¡ìŠµë‹ˆë‹¤. (ë­ë¼ë„ ì—°ê²°!)
        if selected_model is None:
            first_model = my_available_models[0]
            selected_model = genai.GenerativeModel(first_model.name)
            connected_name = f"{first_model.name} (ìë™ ì„ íƒë¨)"

    except Exception as e:
        return None, str(e)

    return selected_model, connected_name

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤í–‰
model, model_name = get_model()

if model is None:
    st.error(f"ğŸ˜­ ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {model_name}\nAPI í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ì‹œë„í•´ì£¼ì„¸ìš”.")
    st.stop()
else:
    # ì„±ê³µí•˜ë©´ ì–´ë–¤ ëª¨ë¸ì„ ì°¾ì•˜ëŠ”ì§€ ì‚¬ì´ë“œë°”ì— í‘œì‹œ
    st.sidebar.success(f"âœ… ë‚´ ì»´í“¨í„° ë§ì¶¤ ì—°ê²°!\nëª¨ë¸ëª…: {model_name}")

# 4. ìë£Œ ìë™ ì½ê¸° í•¨ìˆ˜
@st.cache_data(show_spinner=False)
def load_data():
    folder_path = 'data'
    combined_text = ""
    
    if not os.path.exists(folder_path):
        return ""

    files = os.listdir(folder_path)
    KEYWORDS = ["ë¼ˆ", "ê·¼ìœ¡", "ì†Œí™”", "ì‹¬ì¥", "í˜¸í¡", "ë°°ì„¤", "ë‡Œ", "ì‹ ê²½", "ê°ê°"]

    for filename in files:
        file_path = os.path.join(folder_path, filename)
        try:
            content = ""
            if filename.endswith('.pdf'):
                with open(file_path, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    for page in pdf_reader.pages:
                        content += page.extract_text()
            elif filename.endswith('.docx'):
                doc = Document(file_path)
                for para in doc.paragraphs:
                    content += para.text + "\n"
            elif filename.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            
            if any(k in content for k in KEYWORDS):
                combined_text += f"\n\n--- [ì°¸ê³  ìë£Œ: {filename}] ---\n{content}"
        except Exception:
            pass 

    if len(combined_text) > 60000:
        combined_text = combined_text[:60000] + "\n...(ì´í•˜ ìƒëµ)..."
        
    return combined_text

# 5. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (êµìœ¡ ë° ìœ¤ë¦¬ ê¸°ëŠ¥ ì™„ë¹„)
if "knowledge" not in st.session_state:
    with st.spinner("ì„ ìƒë‹˜ì´ ìë£Œë¥¼ ì±™ê²¨ì˜¤ê³  ìˆì–´ìš”... ğŸ“š"):
        st.session_state.knowledge = load_data()

system_prompt = f"""
ë‹¹ì‹ ì€ ì´ˆë“±í•™êµ 6í•™ë…„ ê³¼í•™ ì„ ìƒë‹˜(ì´ëª¨ì§€: ğŸ§‘â€ğŸ«)ì…ë‹ˆë‹¤.
ì•„ë˜ [í•™ìŠµ ìë£Œ]ì˜ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ í•™ìƒê³¼ ëŒ€í™”í•©ë‹ˆë‹¤.

[í•™ìŠµ ìë£Œ]:
{st.session_state.knowledge}

[âš ï¸ ì¤‘ìš”: ìœ¤ë¦¬ ë° ì•ˆì „ ê°€ì´ë“œë¼ì¸ (ë³´ì•ˆê´€ ê¸°ëŠ¥)]:
1. **ë¹„ì†ì–´ ë° ë¹„ë°© ê¸ˆì§€**: í•™ìƒì´ ìš•ì„¤, ë¹„ì†ì–´, ì¹œêµ¬ë¥¼ ë†€ë¦¬ëŠ” ë§ì„ ì“°ë©´ ì •ì¤‘í•˜ì§€ë§Œ ë‹¨í˜¸í•˜ê²Œ ë‹µë³€ì„ ê±°ì ˆí•˜ê³  ë°”ë¥¸ ë§ì„ ì“°ë„ë¡ ì§€ë„í•˜ì„¸ìš”.
2. **ìœ„í—˜í•œ ì§ˆë¬¸ ì°¨ë‹¨**: í­ë°œë¬¼ ì œì¡°, ìí•´, í­ë ¥, ì•½ë¬¼ ì˜¤ë‚¨ìš© ë“± ìœ„í—˜í•˜ê±°ë‚˜ ë¹„ìœ¤ë¦¬ì ì¸ ì§ˆë¬¸ì—ëŠ” **ì ˆëŒ€ ë‹µí•˜ì§€ ë§ˆì„¸ìš”.**
3. **ëŒ€ì²˜ ë°©ë²•**: "ê·¸ëŸ° ìœ„í—˜í•œ í–‰ë™ì€ í•˜ë©´ ì•ˆ ë¼.", "ìš°ë¦¬ ê³¼í•™ ìˆ˜ì—…ê³¼ ê´€ë ¨ ì—†ëŠ” ë¹„ìœ¤ë¦¬ì ì¸ ë‚´ìš©ì€ ì•Œë ¤ì¤„ ìˆ˜ ì—†ì–´."ë¼ê³  ë§í•˜ê³ , ë‹¤ì‹œ ìš°ë¦¬ ëª¸ì— ëŒ€í•œ í•™ìŠµ ì£¼ì œë¡œ ëŒ€í™”ë¥¼ ìœ ë„í•˜ì„¸ìš”.
4. **ê°œì¸ì •ë³´ ë³´í˜¸**: í•™ìƒì´ ë³¸ì¸ì˜ ì´ë¦„, ì£¼ì†Œ, ì „í™”ë²ˆí˜¸ë¥¼ ë§í•˜ë ¤ í•˜ë©´ "ê°œì¸ì •ë³´ëŠ” ì†Œì¤‘í•˜ë‹ˆê¹Œ ì—¬ê¸°ì— ì ìœ¼ë©´ ì•ˆ ë¼!"ë¼ê³  ì•Œë ¤ì£¼ì„¸ìš”.

[êµìœ¡ì  ëŒ€í™” ë° í–‰ë™ ìˆ˜ì¹™]:
1. **ë§íˆ¬**: ë‹¤ì •í•˜ê³  ì¹œì ˆí•œ ì¡´ëŒ“ë§(í•´ìš”ì²´) ì‚¬ìš©. ì ì ˆí•œ ì´ëª¨ì§€ ì‚¬ìš©ìœ¼ë¡œ ì¹œë°€ê° í˜•ì„±.
2. **ëˆˆë†’ì´ ì„¤ëª…**: ì–´ë ¤ìš´ ì „ë¬¸ ìš©ì–´ ëŒ€ì‹  ì‰¬ìš´ ë¹„ìœ ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. (ì˜ˆ: ì‹¬ì¥ -> íŒí”„, í˜ˆê´€ -> ë„ë¡œ)
3. **ì˜¤ê°œë… êµì •**: í•™ìƒì´ í‹€ë¦° ë‚´ìš©ì„ ë§í•˜ë©´ ë°”ë¡œ ì •ë‹µì„ ì£¼ì§€ ë§ê³ , ë°˜ë¡€ë¥¼ ë“¤ê±°ë‚˜ ì§ˆë¬¸ì„ ë˜ì ¸ ìŠ¤ìŠ¤ë¡œ ê¹¨ë‹«ê²Œ ìœ ë„í•˜ì„¸ìš”.
4. **ë‹¨ê³„ì  íŒíŠ¸(ë¹„ê³„ ì„¤ì •)**: í€´ì¦ˆë‚˜ ì§ˆë¬¸ì— ëŒ€í•´ í•™ìƒì´ ëª¨ë¥¼ ê²½ìš°, íŒíŠ¸ë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ì œê³µí•˜ì—¬ ì‚¬ê³ ë ¥ì„ í‚¤ì›Œì£¼ì„¸ìš”.
5. **ì§ˆë¬¸ ìœ ë„**: ì„¤ëª…ì´ ëë‚œ í›„ì—ëŠ” "í˜¹ì‹œ ë” ê¶ê¸ˆí•œ ê²Œ ìˆë‹ˆ?" ë˜ëŠ” ê´€ë ¨ëœ í¥ë¯¸ë¡œìš´ ì§ˆë¬¸ì„ ë˜ì ¸ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”.
"""

# 6. ëŒ€í™” ì²˜ë¦¬
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ì•ˆë…•! ë°˜ê°€ì›Œ. ì„ ìƒë‹˜ì´ë‘ ìš°ë¦¬ ëª¸ì— ëŒ€í•´ ì¬ë¯¸ìˆê²Œ ì´ì•¼ê¸° ë‚˜ëˆ ë³¼ê¹Œ? í˜¹ì‹œ ê¶ê¸ˆí•œ ì ì´ ìˆë‹ˆ? ğŸ˜Š"}
    ]

for message in st.session_state.messages:
    avatar = "ğŸ§‘â€ğŸ«" if message["role"] == "assistant" else "ğŸ§‘â€ğŸ“"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì´ë‚˜ ëŒ€ë‹µì„ ì…ë ¥í•˜ì„¸ìš”"):
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸ“"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant", avatar="ğŸ§‘â€ğŸ«"):
        msg_box = st.empty()
        try:
            full_prompt = system_prompt + f"\n\ní•™ìƒ ë§: {prompt}"
            response = model.generate_content(full_prompt, stream=True)
            full_response = ""
            for chunk in response:
                full_response += chunk.text
                msg_box.markdown(full_response + "â–Œ")
            msg_box.markdown(full_response)
            st.session_state.messages.append({"role": "model", "content": full_response})     
        except Exception as e:
            msg_box.error(f"ë‹µë³€ì„ ë§Œë“œëŠ” ì¤‘ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”: {e}")